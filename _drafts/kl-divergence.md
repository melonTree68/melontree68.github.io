---
layout: post
title: 'Information Theory Behind the Kullback-Leibler Divergence'
created: '2025-11-11'
categories:
- Machine Learning
tags:
- machine learning
- information theory
description: Why do we use the Kullback-Leibler (KL) divergence as a distance metric between probability distributions? Why do we use the cross-entropy as the loss function in supervised machine learning?
math: true
---
